{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T15:24:56.802540Z",
     "start_time": "2025-01-26T15:24:56.714723Z"
    }
   },
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2\n\nimport torch\nfrom pathlib import Path\n\nfrom sampling.sampling import sample_on_sphere_uniform, sample_conditional\nfrom data.generation import SphereDecoderLinear, SphereDecoderIdentity\nfrom data.encoder import SphereEncoder\nimport invertible_network_utils\n\nimport matplotlib.pyplot as plt\n\nfrom evals.disentanglement import linear_disentanglement, permutation_disentanglement\n\nimport encoders\n\n# Create figures directory\nfigures_dir = Path('figures')\nfigures_dir.mkdir(exist_ok=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02394f-9ff4-420b-ab05-46fce7c311d0",
   "metadata": {},
   "outputs": [],
   "source": "# Setting up the initial experiment\n\nlatent_dimension = 3\noutput_dimension = 3 # Here it must match, else it's not an invertible matrix\nnumber_of_samples = 300\n\ngeneration_process_linear = SphereDecoderLinear(latent_dimension, output_dimension)\ngeneration_process_identity = SphereDecoderIdentity()\n\ngeneration_process_mlp = invertible_network_utils.construct_invertible_mlp(\n        n=3,\n        n_layers=3,\n        act_fct=\"leaky_relu\",\n        cond_thresh_ratio=0.0,\n        n_iter_cond_thresh=25000,\n).to('cpu')\n\nencoder = SphereEncoder()\n\nz_reference = sample_on_sphere_uniform(number_of_samples, latent_dimension)\ndecoded_data = generation_process_identity(z_reference)\n\nencoded_data = encoder(decoded_data)\n\n# Sanity checks\nprint('Maximum squared norm:', (encoded_data ** 2).sum(dim=1).max())\nprint('Minimum squared norm:', (encoded_data ** 2).sum(dim=1).min())\n\ndef visualize_spheres_side_by_side(original_latents, encoded_latents, save_path=None):\n    z_3d = original_latents[:, :3].detach().numpy()\n    encoded_data_3d = encoded_latents[:, :3].detach().numpy()\n    \n    c_original = 0.5 * z_3d[:, 2] + 0.5\n    c_encoded = 0.5 * encoded_data_3d[:, 2] + 0.5\n\n    fig = plt.figure(figsize=(12, 6))\n    \n    # Original points\n    ax1 = fig.add_subplot(121, projection='3d')\n    scatter1 = ax1.scatter(z_3d[:, 0], z_3d[:, 1], z_3d[:, 2], \n                            c=c_original, cmap='viridis', s=20, alpha=0.8)\n    ax1.set_title('Original Latent Points (z)')\n    fig.colorbar(scatter1, ax=ax1, shrink=0.5, aspect=10, label='Color by Z-axis')\n\n    # Encoded points\n    ax2 = fig.add_subplot(122, projection='3d')\n    scatter2 = ax2.scatter(encoded_data_3d[:, 0], encoded_data_3d[:, 1], encoded_data_3d[:, 2], \n                            c=c_encoded, cmap='viridis', s=20, alpha=0.8)\n    ax2.set_title('Encoded Points')\n    fig.colorbar(scatter2, ax=ax2, shrink=0.5, aspect=10, label='Color by Z-axis')\n\n    plt.tight_layout()\n    \n    if save_path:\n        fig.savefig(save_path, dpi=150, bbox_inches='tight')\n    \n    plt.show()\n    return fig\n\nfig = visualize_spheres_side_by_side(z_reference, encoded_data, figures_dir / 'initial_setup.png')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd441364-0fc4-4f6e-ba4f-c0466161160d",
   "metadata": {},
   "outputs": [],
   "source": "kappa_1 = 0.1\nd_fixed = 0 # Initial experiment will allow variation in all dimensions\n\n# Generate augmented latents (ensure `sample_conditional` is defined)\naugmented_z = sample_conditional(z_reference, kappa_1, d_fixed)\n\ndecoded_augmented_data = generation_process_identity(augmented_z)\n\nencoded_augmented_data = encoder(decoded_augmented_data)\n\n# Sanity checks for augmented and encoded data\nprint('Maximum squared norm (Original):', (augmented_z ** 2).sum(dim=1).max().item())\nprint('Minimum squared norm (Original):', (augmented_z ** 2).sum(dim=1).min().item())\nprint('Maximum squared norm (Encoded Augmented):', (encoded_augmented_data ** 2).sum(dim=1).max().item())\nprint('Minimum squared norm (Encoded Augmented):', (encoded_augmented_data ** 2).sum(dim=1).min().item())\n\nfig = visualize_spheres_side_by_side(augmented_z, encoded_augmented_data, figures_dir / 'augmented_data.png')"
  },
  {
   "cell_type": "markdown",
   "id": "90688e32-4eae-4a19-a019-d3de7bbe1cb5",
   "metadata": {},
   "source": [
    "## Training Optimal Encoder\n",
    "\n",
    "In this part of the experiment you need to train an optimal encoder using the SimCLR framework. The goal is to get it to reconstruct a sphere in a setting where all dimensions are allowed to vary.\n",
    "\n",
    "From the paper, they give the following recipie given $2N$ samples: $N$ original samples + $N$ augmentations:\n",
    "\n",
    "1. $s_{i,j} := z_{i}^{T}z_{j} \\ \\forall i,j \\in \\{1,...,2N\\}$ (Here we won't normalize, because the vecors are already on the unit sphere)\n",
    "2. $l(i,j) := -log(\\frac{exp(s_{i,j}/\\tau)}{\\Sigma_{k=1}^{2N} \\mathbb{1}_{[k \\neq i}]exp(s_{i,k}/\\tau)})$\n",
    "3. $\\mathcal{L} := \\frac{1}{2N} \\Sigma_{k=1}^{N} l(2k-1,2k) + l(2k,2k-1)$\n",
    "\n",
    "Here one thing I have omitted was that in their framework $z_{2k} \\sim z_{2k-1}$\n",
    "\n",
    "In this implementation we will use $z_{k} \\sim z_{N + k}$\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "To get a similarity matrix of dot products of vectors, we can simply define $A := [z^{T}, \\tilde{z}]^{T} \\in \\mathbb{R}^{2N \\times 3}$ - a matrix that holds all vectors, and compute the resulting similarity matrix $S = AA^{T}$ using the matrix product.\n",
    "\n",
    "$S_{i,j} = \\Sigma_{k=1}^{3}A_{i,k}A^{T}_{k,j} = \\Sigma_{k=1}^{3}A_{i,k}A_{j,k} = A_{i,1}A_{j,1} + A_{i,2}A_{j,2} + A_{i,3}A_{j,3} = z_{i,1}z_{j,1} + z_{i,2}z_{j,2} + z_{i,3}z_{j,3} = z_{i}^{T}z_{j}$\n",
    "\n",
    "## On loss implementation\n",
    "\n",
    "torch.nn.CrossEntropy for 1 sample takes in a vector of dimension $C$, and a class c, outputs $-log(\\frac{exp(x_{c})}{\\Sigma_{k=1}^{C}exp(x_{k})})$\n",
    "\n",
    "What we need to do here is to first, mask the diagonal with $-\\infty$, since we want the denominator to vanish out with exp(-inf) on the self-similarity term. On the numerator term, I want to take the indices in the following order: (2,1), (1,2), (3,4), (4,3), (5,6), (6,5), etc. (from the paper).\n",
    "\n",
    "The implementation:\n",
    "\n",
    "1. Compute similarity matrix\n",
    "2. Mask the diagonal\n",
    "3. Compute the labels by first doing arrange from 0,..,2N-1, then swap every 2 entries\n",
    "\n",
    "**Correction:** Here we used a different convention than the SimCLR paper. Instead of marking 2k, 2k-1 samples as similar, we mark (k, N + k) samples as similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e5642-447e-4026-bd9f-ac186efc659c",
   "metadata": {},
   "outputs": [],
   "source": "#Using SimCLR framework to find an optimal encoder\n\nfrom simclr.simclr import SimCLRLoss\nfrom experiment_utils.linear import linear_unrotation\nfrom visualization_utils.spheres import scatter3d_sphere\n\ntemperature = 0.1\niterations = 1000\n\nloss_fn = SimCLRLoss(temperature)\nencoder = encoders.get_mlp(\n    n_in=3,\n    n_out=3,\n    layers=[\n        3 * 10,\n        3 * 50,\n        3 * 50,\n        3 * 50,\n        3 * 50,\n        3 * 10,\n    ],\n    output_normalization=\"fixed_sphere\",\n)\nencoder = encoder.to(\"cpu\")\n\n# encoder.train()\noptimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)\n\nbatch_size = 6144\n\n# TODO: Use the same temperatrue when training\n\nfor i in range(iterations):  \n    z = sample_on_sphere_uniform(batch_size, latent_dimension)\n    z_augmented = sample_conditional(z, kappa_1, d_fixed)\n\n    z_neg = sample_on_sphere_uniform(batch_size, latent_dimension)\n\n    optimizer.zero_grad()\n    \n    x = generation_process_identity(z)\n    x_augmented = generation_process_identity(z_augmented)\n    x_neg = generation_process_identity(z_neg)\n        \n    z_reconstructed = encoder(x)\n    z_rec_augmented = encoder(x_augmented)\n    z_rec_neg = encoder(x_neg)\n    \n    l = loss_fn(z_reconstructed, z_rec_augmented, z_rec_neg)    \n    l.backward()\n    optimizer.step()\n\n    if i % 250 == 0:\n        linear_score = linear_disentanglement(z, z_reconstructed, mode=\"r2\")[0][0]\n        (perm_score, _, ), _ = permutation_disentanglement(z, z_reconstructed, mode=\"pearson\",solver=\"munkres\", rescaling=True)\n\n        print('LOSS:', l.item(), 'Linear:', linear_score, 'Permutation:', perm_score, 'Iterations: ', i + 1)\n        \n\nencoder.eval()\n\ntest = sample_on_sphere_uniform(batch_size, latent_dimension)\n\nx = generation_process_mlp(test)\nresult = encoder(x)\n\n# Compare the reference latent samples with the encoded latent samples\nfig = visualize_spheres_side_by_side(test, result, figures_dir / 'training_result.png')\n\n# Unrotation\nz_unrotated = linear_unrotation(test, result)\n\nfig = scatter3d_sphere(plt, test.cpu(), z_unrotated, s=10, a=.8)\nfig.savefig(figures_dir / 'unrotated_result.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"All figures saved to {figures_dir}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b523f-8bff-4b42-bad5-686a77524a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d88341-fa2a-49be-9be7-84924afa6352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafa0f9-00f5-4099-b32b-b83bea7f6cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}